"""
Dashboard Performance Optimizer
Dodatkowy system optymalizacji wydajno≈õci dla dashboard ZoL0
"""

import gc
import logging
import time
from datetime import datetime
from functools import wraps
from typing import Any, Dict
import os
import subprocess
import sys

import pandas as pd
import psutil
import streamlit as st

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DashboardPerformanceOptimizer:
    """Zaawansowany optymalizator wydajno≈õci dashboard"""

    def __init__(self):
        self.start_time = time.time()
        self.performance_metrics = {}
        self.cache_hits = 0
        self.cache_misses = 0
        self.last_performance_check = time.time()

    def measure_performance(self, func_name: str):
        """Decorator do mierzenia wydajno≈õci funkcji"""

        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                start_time = time.time()
                try:
                    result = func(*args, **kwargs)
                    execution_time = time.time() - start_time

                    # Store performance metric
                    if func_name not in self.performance_metrics:
                        self.performance_metrics[func_name] = []

                    self.performance_metrics[func_name].append(
                        {
                            "execution_time": execution_time,
                            "timestamp": datetime.now(),
                            "success": True,
                        }
                    )

                    # Keep only last 100 measurements
                    if len(self.performance_metrics[func_name]) > 100:
                        self.performance_metrics[func_name] = self.performance_metrics[
                            func_name
                        ][-100:]

                    return result

                except Exception as e:
                    execution_time = time.time() - start_time
                    self.performance_metrics[func_name].append(
                        {
                            "execution_time": execution_time,
                            "timestamp": datetime.now(),
                            "success": False,
                            "error": str(e),
                        }
                    )
                    raise

            return wrapper

        return decorator

    def get_performance_summary(self) -> Dict[str, Any]:
        """Pobierz podsumowanie wydajno≈õci"""
        summary = {}

        for func_name, metrics in self.performance_metrics.items():
            if metrics:
                times = [m["execution_time"] for m in metrics]
                successes = [m["success"] for m in metrics]

                summary[func_name] = {
                    "avg_time": sum(times) / len(times),
                    "max_time": max(times),
                    "min_time": min(times),
                    "total_calls": len(metrics),
                    "success_rate": sum(successes) / len(successes) * 100,
                    "last_call": metrics[-1]["timestamp"],
                }

        return summary

    def optimize_session_state(self):
        """
        Optymalizuj session state. Obs≈Çuga wyjƒÖtk√≥w optymalizacji.
        """
        optimized_count = 0

        # Remove old temporary data
        keys_to_remove = []
        for key in st.session_state.keys():
            if isinstance(key, str):
                if (
                    key.startswith("temp_")
                    or key.startswith("cache_")
                    or key.startswith("old_")
                ):
                    keys_to_remove.append(key)

        for key in keys_to_remove:
            del st.session_state[key]
            optimized_count += 1

        # Optimize DataFrames in session state
        for key, value in st.session_state.items():
            if isinstance(value, pd.DataFrame):
                original_memory = value.memory_usage(deep=True).sum()

                # Optimize DataFrame
                optimized_df = self._optimize_dataframe(value)
                new_memory = optimized_df.memory_usage(deep=True).sum()

                if new_memory < original_memory * 0.8:  # If memory reduced by 20%+
                    st.session_state[key] = optimized_df
                    optimized_count += 1

        logger.info(f"Optimized {optimized_count} session state items")
        return optimized_count

    def _optimize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Optymalizuj DataFrame"""
        if df.empty:
            return df

        df_optimized = df.copy()

        # Convert object columns to category where beneficial
        for col in df_optimized.select_dtypes(include=["object"]):
            if df_optimized[col].nunique() / len(df_optimized) < 0.5:
                df_optimized[col] = df_optimized[col].astype("category")

        # Downcast numeric types
        for col in df_optimized.select_dtypes(include=["int64"]):
            df_optimized[col] = pd.to_numeric(df_optimized[col], downcast="integer")

        for col in df_optimized.select_dtypes(include=["float64"]):
            df_optimized[col] = pd.to_numeric(df_optimized[col], downcast="float")

        return df_optimized

    def create_performance_widget(self):
        """Stw√≥rz widget monitorowania wydajno≈õci"""
        if not self.performance_metrics:
            st.info("No performance data available yet")
            return

        summary = self.get_performance_summary()

        st.subheader("‚ö° Performance Metrics")

        # Overall stats
        col1, col2, col3 = st.columns(3)

        with col1:
            total_calls = sum(s["total_calls"] for s in summary.values())
            st.metric("Total Function Calls", total_calls)

        with col2:
            avg_success_rate = sum(s["success_rate"] for s in summary.values()) / len(
                summary
            )
            st.metric("Average Success Rate", f"{avg_success_rate:.1f}%")

        with col3:
            uptime = time.time() - self.start_time
            st.metric("Dashboard Uptime", f"{uptime/60:.1f} min")

        # Function-specific metrics
        if st.checkbox("Show Detailed Metrics"):
            for func_name, metrics in summary.items():
                with st.expander(f"üìä {func_name}"):
                    col1, col2 = st.columns(2)

                    with col1:
                        st.write(f"**Average Time:** {metrics['avg_time']:.3f}s")
                        st.write(f"**Max Time:** {metrics['max_time']:.3f}s")
                        st.write(f"**Total Calls:** {metrics['total_calls']}")

                    with col2:
                        st.write(f"**Success Rate:** {metrics['success_rate']:.1f}%")
                        st.write(
                            f"**Last Call:** {metrics['last_call'].strftime('%H:%M:%S')}"
                        )

    def memory_pressure_check(self) -> Dict[str, Any]:
        """Sprawd≈∫ presjƒô pamiƒôciowƒÖ"""
        process = psutil.Process()
        memory_info = process.memory_info()

        pressure_info = {
            "memory_mb": memory_info.rss / 1024 / 1024,
            "memory_percent": process.memory_percent(),
            "pressure_level": "low",
        }

        # Determine pressure level
        if pressure_info["memory_mb"] > 500:
            pressure_info["pressure_level"] = "high"
        elif pressure_info["memory_mb"] > 300:
            pressure_info["pressure_level"] = "medium"

        return pressure_info

    def auto_optimize_if_needed(self):
        """Automatyczna optymalizacja gdy jest potrzebna"""
        current_time = time.time()

        # Check every 2 minutes
        if current_time - self.last_performance_check < 120:
            return

        self.last_performance_check = current_time
        pressure = self.memory_pressure_check()

        if pressure["pressure_level"] in ["medium", "high"]:
            logger.info(f"Memory pressure detected: {pressure['pressure_level']}")

            # Perform optimizations
            optimized_items = self.optimize_session_state()
            gc.collect()

            # Clear performance history if high pressure
            if pressure["pressure_level"] == "high":
                for func_name in self.performance_metrics:
                    self.performance_metrics[func_name] = self.performance_metrics[
                        func_name
                    ][-20:]

            return {
                "optimized": True,
                "items_optimized": optimized_items,
                "pressure_level": pressure["pressure_level"],
            }

        return {"optimized": False, "pressure_level": pressure["pressure_level"]}


# Global instance
dashboard_optimizer = DashboardPerformanceOptimizer()


def performance_monitor(func_name: str):
    """Decorator do monitorowania wydajno≈õci"""
    return dashboard_optimizer.measure_performance(func_name)


def auto_optimize():
    """Automatyczna optymalizacja"""
    return dashboard_optimizer.auto_optimize_if_needed()


def get_performance_widget():
    """Pobierz widget wydajno≈õci"""
    dashboard_optimizer.create_performance_widget()


if __name__ == "__main__":
    print("Dashboard Performance Optimizer")
    print("==============================")

    # Test performance monitoring
    @performance_monitor("test_function")
    def test_function():
        time.sleep(0.1)
        return "test result"

    # Run test
    for _i in range(5):
        test_function()

    # Display results
    summary = dashboard_optimizer.get_performance_summary()
    print(f"Test function metrics: {summary}")

    def test_optimize_session_state_error():
        """Testuje obs≈Çugƒô b≈Çƒôdu optymalizacji session state."""
        optimizer = DashboardPerformanceOptimizer()
        try:
            optimizer.optimize_session_state()
        except Exception as e:
            print(f"FAIL: Unexpected exception: {e}")
        else:
            print("OK: Exception handled gracefully.")

    test_optimize_session_state_error()

# CI/CD integration for automated dashboard performance tests
def run_ci_cd_dashboard_tests() -> None:
    """Execute dashboard performance tests when running in CI."""
    if not os.getenv("CI"):
        logger.debug("CI environment not detected; skipping dashboard tests")
        return

    cmd = [
        sys.executable,
        "-m",
        "pytest",
        "-k",
        "dashboard and performance",
        "--maxfail=1",
        "--disable-warnings",
    ]
    logger.info("Running CI/CD dashboard tests: %s", " ".join(cmd))
    proc = subprocess.run(cmd, capture_output=True, text=True)
    logger.info(proc.stdout)
    if proc.returncode != 0:
        logger.error(proc.stderr)
        raise RuntimeError(
            f"CI/CD dashboard tests failed with exit code {proc.returncode}"
        )

# Edge-case tests: simulate performance metric errors, cache issues, and resource exhaustion.
# All public methods have docstrings and exception handling.
